{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1185a6",
   "metadata": {},
   "source": [
    "# Lab 15 — Regularization for MLPs\n",
    "In this lab, you will investigate how different regularization techniques affect the\n",
    "training dynamics and generalization performance of a simple neural network.\n",
    "\n",
    "We will implement and compare:\n",
    "- **Dropout** — randomly dropping neurons during training\n",
    "- **L1 and L2 regularization** — penalizing large weights  \n",
    "- **Early stopping** — stopping training when validation performance degrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47caed5",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We will use a noisy 2D classification dataset to illustrate overfitting and the effects of regularization. The training data has high noise to encourage overfitting, while the validation data is cleaner to test generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b247ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.25, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert to torch tensors\n",
    "x_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "print(f\"Training set: {x_train.shape}, Validation set: {x_val.shape}\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\"coolwarm\", alpha=0.7, edgecolor='k')\n",
    "plt.title(\"Training data with noisy boundary\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e67599",
   "metadata": {},
   "source": [
    "## Network Architecture and Forward Pass\n",
    "\n",
    "We'll implement a neural network with:\n",
    "- **Input layer**: 2 neurons ($x_1$, $x_2$)\n",
    "- **Hidden layer 1**: 20 neurons with tanh activation\n",
    "- **Hidden layer 2**: 10 neurons with tanh activation\n",
    "- **Output layer**: 1 neuron with sigmoid activation (for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: Tensor) -> Tensor:\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def tanh(x: Tensor) -> Tensor:\n",
    "    return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture: [input_size, hidden1, hidden2, ..., output_size]\n",
    "ARCHITECTURE = [2, 20, 10, 1]\n",
    "\n",
    "def count_parameters(layer_sizes):\n",
    "    \"\"\"Count total parameters for a given architecture.\"\"\"\n",
    "    total = 0\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        total += layer_sizes[i] * layer_sizes[i+1] + layer_sizes[i+1]\n",
    "    return total\n",
    "\n",
    "def initialize_parameters(layer_sizes, scale=0.3):\n",
    "    \"\"\"Initialize parameters for the network.\"\"\"\n",
    "    n_params = count_parameters(layer_sizes)\n",
    "    return torch.randn(n_params) * scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26670e23",
   "metadata": {},
   "source": [
    "### Ex. 1: Forward Pass with Dropout (3P)\n",
    "\n",
    "Implement a forward pass with dropout support. Dropout randomly drops neurons during training to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_dropout(\n",
    "    x: Tensor,\n",
    "    params: Tensor,\n",
    "    layer_sizes: list[int],\n",
    "    dropout_rate: float = 0.0,\n",
    "    training: bool = True\n",
    ") -> tuple[Tensor, dict]:\n",
    "    \"\"\"\n",
    "    Forward pass through network with dropout support.\n",
    "    \n",
    "    Args:\n",
    "        x: Input data (N x input_dim)\n",
    "        params: All network parameters as a flat tensor\n",
    "        layer_sizes: Architecture [input_size, hidden1, ..., output_size]\n",
    "        dropout_rate: Probability of dropping a neuron\n",
    "        training: Whether in training mode\n",
    "    \n",
    "    Returns:\n",
    "        output: Network predictions\n",
    "        cache: Dictionary storing intermediate values for backprop\n",
    "    \"\"\"\n",
    "    cache = {'layer_sizes': layer_sizes}\n",
    "    activations = [x]\n",
    "    dropout_masks = []\n",
    "    \n",
    "    # Unpack parameters dynamically\n",
    "    idx = 0\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        n_in, n_out = layer_sizes[i], layer_sizes[i+1]\n",
    "        \n",
    "        # Extract bias and weight for this layer\n",
    "        b = params[idx:idx + n_out]\n",
    "        idx += n_out\n",
    "        \n",
    "        W = params[idx:idx + n_in * n_out].view(n_in, n_out)\n",
    "        idx += n_in * n_out\n",
    "        \n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    \n",
    "    # Forward pass through layers\n",
    "    for i in range(len(weights)):\n",
    "        W, b = weights[i], biases[i]\n",
    "        \n",
    "        # Linear transformation\n",
    "        z_in = activations[-1] @ W + b\n",
    "        cache[f'z{i}_in'] = z_in\n",
    "        \n",
    "        # Activation function (before dropout)\n",
    "        if i < len(weights) - 1:  # Hidden layers use tanh\n",
    "            z_out_pre_dropout = tanh(z_in)\n",
    "        else:  # Output layer uses sigmoid\n",
    "            z_out_pre_dropout = sigmoid(z_in)\n",
    "        \n",
    "        # Store pre-dropout activation\n",
    "        cache[f'z{i}_out_pre_dropout'] = z_out_pre_dropout\n",
    "        \n",
    "        # TODO: Apply dropout to hidden layers only\n",
    "        # Hint: Use inverted dropout with scaling factor 1/(1-dropout_rate)\n",
    "        # Hint: Only apply dropout during training (check the 'training' flag)\n",
    "        z_out = z_out_pre_dropout  # Replace this line with your implementation\n",
    "        \n",
    "        cache[f'z{i}_out'] = z_out\n",
    "        cache[f'W{i}'] = W\n",
    "        cache[f'b{i}'] = b\n",
    "        activations.append(z_out)\n",
    "    \n",
    "    cache['activations'] = activations\n",
    "    \n",
    "    return activations[-1], cache\n",
    "\n",
    "# Test\n",
    "params = initialize_parameters(ARCHITECTURE)\n",
    "predictions, cache = predict_with_dropout(x_train, params, ARCHITECTURE, dropout_rate=0.0, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d66790",
   "metadata": {},
   "source": [
    "## Exercise 2 — Loss Function with Regularization (2P)\n",
    "\n",
    "Extend the binary cross-entropy loss to include L1 and L2 regularization.\n",
    "\n",
    "**Regularization formulas**:\n",
    "- **L1**: $\\lambda_1 \\sum |w_i|$ (encourages sparsity)\n",
    "- **L2**: $\\lambda_2 \\sum w_i^2$ (encourages small weights)\n",
    "\n",
    "Note: We only regularize weights, not biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c8325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_with_regularization(\n",
    "    target: Tensor,\n",
    "    pred: Tensor,\n",
    "    params: Tensor,\n",
    "    layer_sizes: list[int],\n",
    "    lambda_l1: float = 0.0,\n",
    "    lambda_l2: float = 0.0\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute binary cross-entropy loss with L1/L2 regularization.\n",
    "    \n",
    "    Args:\n",
    "        target: True labels\n",
    "        pred: Predicted probabilities\n",
    "        params: Network parameters as flat tensor\n",
    "        layer_sizes: Architecture [input_size, hidden1, ..., output_size]\n",
    "        lambda_l1: L1 regularization strength\n",
    "        lambda_l2: L2 regularization strength\n",
    "    \"\"\"\n",
    "    # Ensure target and pred have matching shapes\n",
    "    if target.dim() == 1 and pred.dim() == 2:\n",
    "        target = target.unsqueeze(1)\n",
    "    elif target.dim() == 2 and pred.dim() == 1:\n",
    "        pred = pred.unsqueeze(1)\n",
    "    \n",
    "    # Data loss (binary cross-entropy)\n",
    "    data_loss = -torch.mean(\n",
    "        target * torch.log(pred + 1e-15) + \n",
    "        (1 - target) * torch.log(1 - pred + 1e-15)\n",
    "    )\n",
    "    \n",
    "    # TODO: Add L1 and L2 regularization to weights \n",
    "    \n",
    "    return data_loss  # Replace this with total_loss including regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766f396",
   "metadata": {},
   "source": [
    "## Exercise 3 — Backpropagation with Dropout and Regularization (3P)\n",
    "\n",
    "Implement gradient computation with support for dropout and L1/L2 regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_with_regularization(\n",
    "    target: Tensor,\n",
    "    cache: dict,\n",
    "    params: Tensor,\n",
    "    lambda_l1: float = 0.0,\n",
    "    lambda_l2: float = 0.0,\n",
    "    dropout_rate: float = 0.0\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute gradients via backpropagation with dropout and regularization.\n",
    "    Works for any architecture defined in cache['layer_sizes'].\n",
    "    \"\"\"\n",
    "    layer_sizes = cache['layer_sizes']\n",
    "    activations = cache['activations']\n",
    "    n_layers = len(layer_sizes) - 1\n",
    "    N = activations[0].shape[0]\n",
    "    \n",
    "    # Initialize gradient list\n",
    "    gradients = []\n",
    "    \n",
    "    # Backprop through loss and output layer\n",
    "    fout = activations[-1]\n",
    "    \n",
    "    # Ensure target has same shape as fout to prevent broadcasting issues\n",
    "    if target.dim() == 1:\n",
    "        target = target.unsqueeze(1)\n",
    "    \n",
    "    dL_dfout = -target / (fout + 1e-15) + (1 - target) / (1 - fout + 1e-15)\n",
    "    \n",
    "    # Output layer uses sigmoid - derivative is fout * (1 - fout)\n",
    "    dfout_dfin = fout * (1 - fout)\n",
    "    \n",
    "    dL_dz = dL_dfout * dfout_dfin\n",
    "    \n",
    "    # Backpropagate through all layers (from output to input)\n",
    "    for i in range(n_layers - 1, -1, -1):\n",
    "        W = cache[f'W{i}']\n",
    "        z_prev = activations[i]\n",
    "        \n",
    "        # Gradient w.r.t. bias and weights\n",
    "        dL_db = torch.mean(dL_dz, dim=0)\n",
    "        dL_dW = (z_prev.T @ dL_dz) / N\n",
    "        \n",
    "        # TODO: Add regularization gradients to weights\n",
    "        \n",
    "        # Store gradients (in reverse order, we'll reverse later)\n",
    "        gradients.append((dL_db, dL_dW))\n",
    "        \n",
    "        # Backprop to previous layer (if not input layer)\n",
    "        if i > 0:\n",
    "            dL_dz_prev = dL_dz @ W.T\n",
    "            \n",
    "            # TODO: Apply dropout mask to gradients\n",
    "            # Hint: Don't forget to scale by 1/(1-dropout_rate) for inverted dropout\n",
    "            \n",
    "            # Apply activation derivative (tanh for hidden layers)\n",
    "            z_prev_in = cache[f'z{i-1}_in']\n",
    "            dz_prev_out = 1.0 - torch.tanh(z_prev_in)**2\n",
    "            \n",
    "            # Element-wise multiplication\n",
    "            dL_dz = dL_dz_prev * dz_prev_out\n",
    "    \n",
    "    # Reverse gradients to match parameter order and flatten\n",
    "    gradients = gradients[::-1]\n",
    "    grad_flat = []\n",
    "    for dL_db, dL_dW in gradients:\n",
    "        grad_flat.append(dL_db)\n",
    "        grad_flat.append(dL_dW.flatten())\n",
    "    \n",
    "    return torch.cat(grad_flat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5705a3",
   "metadata": {},
   "source": [
    "## Exercise 4 — Training with Early Stopping (2P)\n",
    "\n",
    "Implement a training loop that monitors validation performance and stops when validation loss stops improving with a patience counter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(\n",
    "    x_train: Tensor,\n",
    "    y_train: Tensor,\n",
    "    x_val: Tensor,\n",
    "    y_val: Tensor,\n",
    "    layer_sizes: list[int],\n",
    "    learning_rate: float = 0.5,\n",
    "    max_steps: int = 1000,\n",
    "    dropout_rate: float = 0.0,\n",
    "    lambda_l1: float = 0.0,\n",
    "    lambda_l2: float = 0.0,\n",
    "    patience: int = 50,\n",
    "    verbose: bool = False\n",
    ") -> tuple[Tensor, dict]:\n",
    "    \"\"\"\n",
    "    Train the network with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        layer_sizes: Network architecture [input_size, hidden1, ..., output_size]\n",
    "    \n",
    "    Returns:\n",
    "        best_params: Parameters achieving best validation loss\n",
    "        history: Dictionary with training and validation losses\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    params = initialize_parameters(layer_sizes)\n",
    "    \n",
    "    # TODO: Implement early stopping logic\n",
    "    # Hint: Track best_val_loss, best_params, and epochs_without_improvement\n",
    "    # Hint: Create history dict with 'train_loss' and 'val_loss' lists\n",
    "    \n",
    "    best_params = params.clone()\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    # Your implementation here\n",
    "    \n",
    "    return best_params, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cff95",
   "metadata": {},
   "source": [
    "## Compare Regularization Techniques\n",
    "\n",
    "Train models with different regularization strategies and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different regularization strategies\n",
    "params_baseline, history_baseline = train_network(\n",
    "    x_train, y_train_t, x_val, y_val_t,\n",
    "    layer_sizes=ARCHITECTURE,\n",
    "    learning_rate=0.5, max_steps=2000, patience=500\n",
    ")\n",
    "\n",
    "params_dropout, history_dropout = train_network(\n",
    "    x_train, y_train_t, x_val, y_val_t,\n",
    "    layer_sizes=ARCHITECTURE,\n",
    "    learning_rate=0.5, max_steps=2000, dropout_rate=0.15, patience=200\n",
    ")\n",
    "\n",
    "params_l1, history_l1 = train_network(\n",
    "    x_train, y_train_t, x_val, y_val_t,\n",
    "    layer_sizes=ARCHITECTURE,\n",
    "    learning_rate=0.5, max_steps=2000, lambda_l1=0.002, patience=200\n",
    ")\n",
    "\n",
    "params_l2, history_l2 = train_network(\n",
    "    x_train, y_train_t, x_val, y_val_t,\n",
    "    layer_sizes=ARCHITECTURE,\n",
    "    learning_rate=0.5, max_steps=2000, lambda_l2=0.005, patience=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4950b",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Plot training curves to compare the different regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "models = [\n",
    "    (\"Baseline (No Regularization)\", history_baseline),\n",
    "    (\"Dropout (rate=0.15)\", history_dropout),\n",
    "    (\"L1 Regularization (λ=0.002)\", history_l1),\n",
    "    (\"L2 Regularization (λ=0.005)\", history_l2)\n",
    "]\n",
    "\n",
    "for idx, (name, history) in enumerate(models):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.plot(history['train_loss'], label='Training Loss', alpha=0.7)\n",
    "    ax.plot(history['val_loss'], label='Validation Loss', alpha=0.7)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(name)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20da7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(ax, x_data, y_data, params, layer_sizes, title):\n",
    "    \"\"\"Plot decision boundary for a trained network.\"\"\"\n",
    "    # Create grid\n",
    "    grid_range = torch.linspace(x_data.min()-0.5, x_data.max()+0.5, 100)\n",
    "    grid_x, grid_y = torch.meshgrid(grid_range, grid_range, indexing='xy')\n",
    "    grid_data = torch.stack([grid_x.flatten(), grid_y.flatten()]).T\n",
    "    \n",
    "    # Get predictions\n",
    "    pred, _ = predict_with_dropout(\n",
    "        grid_data, params, layer_sizes,\n",
    "        dropout_rate=0.0, training=False\n",
    "    )\n",
    "    pred = pred.view(grid_x.shape).detach().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    ax.contourf(grid_x.numpy(), grid_y.numpy(), pred, cmap='RdYlBu_r', alpha=0.6)\n",
    "    ax.scatter(x_data[:, 0], x_data[:, 1], c=y_data, cmap='coolwarm', \n",
    "               edgecolor='k', s=50, alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "\n",
    "# Plot all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "model_params = [\n",
    "    (\"Baseline\", params_baseline),\n",
    "    (\"Dropout\", params_dropout),\n",
    "    (\"L1 Regularization\", params_l1),\n",
    "    (\"L2 Regularization\", params_l2)\n",
    "]\n",
    "\n",
    "for idx, (name, params) in enumerate(model_params):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    plot_decision_boundary(ax, x_val, y_val_t.numpy(), params, ARCHITECTURE, name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
